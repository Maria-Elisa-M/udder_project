# split the files in batches of 200 files each to run a mutltijob in CHTC


import os 
import pandas as pd
import shutil
import json


def create_dir(path):
    if not os.path.exists(path):
        os.mkdir(path)

dirpath = r'C:\Users\marie\rep_codes\udder_project\udder_processing'
pcd_path = os.path.join(dirpath, "point_clouds")
kp_path = os.path.join(pcd_path, "keypoints")
dir_list = ["keypoints","quarters"]
wd = os.getcwd()
filenames = os.listdir(kp_path)
job_list = pd.read_csv("job_idx.csv", header = None, index_col = 0)


for i in range(len(job_list))[1:]:
    job = str(i)
    job_path = os.path.join(wd, job)
    start_i = job_list.iloc[i][1]
    end_i = job_list.iloc[i][2]
    if end_i >len(filenames):
        job_files = filenames[start_i:]
    else:
        job_files = filenames[start_i:end_i]
    create_dir(job)
    job_pc_path = os.path.join(job_path, "point_clouds")
    create_dir(job_pc_path)
    create_dir(os.path.join(job_pc_path, "teat"))
    create_dir(os.path.join(job_path, "features_dict"))
    for dirname in dir_list:
        src = os.path.join(pcd_path, dirname)
        dest = os.path.join(job_pc_path, dirname)
        create_dir(dest)
        for file in job_files:
            src_path  = os.path.join(src, file)
            dest_path  = os.path.join(dest, file)
            shutil.move(src_path, dest_path)


folder_list = os.listdir("jobs")





job_file_dict = {}
for i in range(len(job_list)): 
    start_i = job_list.iloc[i][1]
    end_i = job_list.iloc[i][2]
    
    if end_i >len(filenames):
        job_files = filenames[start_i:]
    else:
        job_files = filenames[start_i:end_i]
    job_file_dict[i] = job_files


with open(os.path.join(wd, "job_file_dict.json"), 'w') as f:
        json.dump(job_file_dict, f)



